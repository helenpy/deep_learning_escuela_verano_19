{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos Sequence-to-Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta el momento hemos trabajado con dos tipos de procesamiento de secuencia: el caso donde a cada elemento de la secuencia se le asigna una etiqueta y el caso donde a una secuencia se le asigna un valor, ya sea categórico o contínuo. Estos son casos útiles y comunes. Sin embargo, falta un caso general. Y este es encontrar una secuencia a partir de otra secuencia, las dos de tamaño arbitrario. A estos modelos se les llama sequence-to-sequence o seq2seq. \n",
    "\n",
    "En este ejercicio vamos a implementar dos variantes del mismo. Para ejemplificar su uso, ¡vamos a usar un dataset multilingue! Le enseñaremos a nuestro modelo a conjugar palabras. En el ejemplo lo haremos en tres idiomas, pero el dataset original contiene 100 idiomas con los cuáles se puede probar. Para mas información puede consultar el Sharedtask Sigmorphon en la sigueinte dirección:\n",
    "\n",
    "https://github.com/sigmorphon/conll2018\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparar los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomaremos el ejemplo de español primero. Dado que ya entendemos el concepto de preparar datos de ocaciones anteriores pondemos todo el proceso en una clase, que puede ser reusada de manera genérica para diferentes idiomas. Sin embargo, los datos esta vez son diferentes a lo que ya habíamos visto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, Dense\n",
    "from tensorflow.keras.utils import to_categorical, get_file\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos los conjuntos de datos que vamos a usar. En este caso están accesibles libremente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file\n",
    "path_to_train_low = get_file(\n",
    "    'spanish-train-low', origin='https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/all/spanish-train-low')\n",
    "path_to_train_medium = get_file(\n",
    "    'spanish-train-medium', origin='https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/all/spanish-train-medium')\n",
    "path_to_dev = get_file(\n",
    "    'spanish-dev', origin='https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/all/spanish-dev')\n",
    "path_to_test = get_file(\n",
    "    'spanish-test', origin='https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/all/spanish-test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente preparamos los datos para que puedan ser ingresados a la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> <V> <NEG> <IMP> <3> <PL> r e i t e r a r <EOS>\n",
      "<BOS> n o   r e i t e r e n <EOS>\n"
     ]
    }
   ],
   "source": [
    "input_to_index = {\"PAD\":0,\"EOS\":1, \"BOS\":2, \"OOV\":3}\n",
    "output_to_index = {\"PAD\":0,\"EOS\":1, \"BOS\":2,\"OOV\":3}\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    data = io.open(path, encoding='UTF-8').read().strip().splitlines()\n",
    "    symbols = list()\n",
    "    input_chars = list()\n",
    "    output_chars = list()\n",
    "    final_data = list()\n",
    "    for line in data:\n",
    "        line =  line.split('\\t')\n",
    "        line[2] = line[2].split(\";\")\n",
    "        line[1] = list(line[1])\n",
    "        line[0] = list(line[0])\n",
    "        tags = list()\n",
    "        for tag in line[2]:\n",
    "            tags.append(\"<\"+tag+\">\")\n",
    "        \n",
    "        input_string = \"<BOS> \" + \" \".join(tags) +\" \" + \" \".join(line[0]) + \" <EOS>\"\n",
    "        output_string = \"<BOS> \" + \" \".join(line[1]) + \" <EOS>\"\n",
    "        \n",
    "        final_data.append((input_string, output_string))\n",
    "    \n",
    "    return zip(*final_data)\n",
    "\n",
    "train_input_med, train_output_med = load_data(path_to_train_medium)\n",
    "train_input_low, train_output_low  = load_data(path_to_train_low)\n",
    "dev_input, dev_output = load_data(path_to_dev)\n",
    "test_input, test_output = load_data(path_to_test)\n",
    "\n",
    "print(train_input_med[0])\n",
    "print(train_output_med[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este problema en particular agregmos una serie de tags que an a caracterizar la información morfológica con la cual queremos conjugar nuestra raíz verbal. Es de notar que también usamos los tags de Begin of sentece y end of sentence. Para el modelo seq2seq esto es importante, dado que va a iniciar a generar salida a partir de un BOS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a extraer nuevamente nuestro vocabulario para poder darle un índice a cada tag o caracter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<OOV>': 1, 'o': 2, 'h': 3, '<3>': 4, '<2>': 5, '<1>': 6, '<POS>': 7, 'f': 8, '<PL>': 9, 'l': 10, 'ü': 11, '<PST>': 12, 'c': 13, '<COND>': 14, 'd': 15, 'v': 16, '<NEG>': 17, 'n': 18, 'ñ': 19, '<SG>': 20, 'j': 21, '<V.CVB>': 22, '<SBJV>': 23, '<V>': 24, '<MASC>': 25, '<NFIN>': 26, '<BOS>': 27, '<IMP>': 28, 'i': 29, 'í': 30, '<EOS>': 31, 'b': 32, '<PRS>': 33, 'm': 34, '<IPFV>': 35, 'z': 36, 'y': 37, 's': 38, '<V.PTCP>': 39, '<FEM>': 40, 'e': 41, 'u': 42, '<FUT>': 43, '<IND>': 44, 'a': 45, 'x': 46, 'p': 47, 't': 48, 'r': 49, '<LGSPEC1>': 50, 'g': 51, '<PFV>': 52, 'q': 53}\n",
      "{'<PAD>': 0, '<OOV>': 1, 'o': 2, 'h': 3, 'ó': 4, 'f': 5, 'l': 6, 'ü': 7, 'c': 8, 'd': 9, 'v': 10, 'n': 11, 'ñ': 12, 'j': 13, '<BOS>': 14, 'i': 15, 'í': 16, '<EOS>': 17, 'b': 18, 'm': 19, 'y': 20, 'z': 21, 'é': 22, 's': 23, 'e': 24, 'u': 25, 'á': 26, 'ú': 27, 'a': 28, 'x': 29, 'p': 30, 't': 31, 'r': 32, 'g': 33, 'q': 34}\n"
     ]
    }
   ],
   "source": [
    "def get_index(data):\n",
    "    word_to_index = {\"<PAD>\":0, \"<OOV>\":1}\n",
    "    index_to_word = {0:\"<PAD>\", 1:\"<OOV>\"}\n",
    "\n",
    "    voc = list()\n",
    "    for line in data:\n",
    "        voc += line.split()\n",
    "    voc = list(set(voc))\n",
    "    for word in voc:\n",
    "        word_to_index[word] = len(word_to_index)\n",
    "        index_to_word[len(index_to_word)] = word\n",
    "\n",
    "    return word_to_index, index_to_word\n",
    "\n",
    "input_to_index, index_to_input = get_index(train_input_med)\n",
    "output_to_index, index_to_output = get_index(train_output_med)\n",
    "\n",
    "print(input_to_index)\n",
    "print(output_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y convertimos todas nuestras entradas a enteros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> <V> <NEG> <IMP> <3> <PL> r e i t e r a r <EOS>\n",
      "[27 24 17 28  4  9 49 41 29 48 41 49 45 49 31  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def to_ints(data, token_to_int):\n",
    "    new_data = list()\n",
    "    for line in data:\n",
    "        new_line = list()\n",
    "        for word in line.split():\n",
    "            if not word in token_to_int.keys():\n",
    "                new_line.append(token_to_int[\"<OOV>\"])\n",
    "                print(\"not seen\", word)\n",
    "            else:\n",
    "                new_line.append(token_to_int[word])\n",
    "        new_data.append(new_line)\n",
    "    \n",
    "    return(new_data)\n",
    "\n",
    "train_med_X = to_ints(train_input_med, input_to_index)\n",
    "train_med_Y = to_ints(train_output_med, output_to_index)\n",
    "\n",
    "dev_X = to_ints(dev_input, input_to_index)\n",
    "dev_Y = to_ints(dev_output, output_to_index)\n",
    "\n",
    "train_med_X = tf.keras.preprocessing.sequence.pad_sequences(train_med_X, padding='post')\n",
    "train_med_Y = tf.keras.preprocessing.sequence.pad_sequences(train_med_Y, padding='post')\n",
    "\n",
    "print(train_input_med[0])\n",
    "print(train_med_X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparamos nuestros datos para ser usados por el tipo de datos de tensorflow Fataset y definimos nuestros hyper parámetros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(train_med_X)\n",
    "BATCH_SIZE = 20\n",
    "MAX_LENGTH = 40\n",
    "steps_per_epoch = len(train_med_X)//BATCH_SIZE\n",
    "embedding_dim = 100\n",
    "units = 1024\n",
    "vocab_inp_size = len(input_to_index)\n",
    "vocab_tar_size = len(output_to_index)\n",
    "SAVE_EACH = 2\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_med_X, train_med_Y)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El modelo encoder-decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definamos el encoder. En este caso para tener mayor control sobre el comportamiento del encoder vamos usar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora usamos el modelo de attención de Bahdanau, tal como hemos discutido en la parte teórica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values):\n",
    "\n",
    "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "    score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "    \n",
    "    return context_vector, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora definiremos el decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "    x = self.embedding(x)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "    output, state = self.gru(x)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos nuestra función de pérdida personalizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos los objetos de encoder y decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos los mejores modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una función de distancia que será útil en la evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(str1, str2):\n",
    "    \"\"\"Simple Levenshtein implementation for evalm.\"\"\"\n",
    "    m = np.zeros([len(str2)+1, len(str1)+1])\n",
    "    for x in range(1, len(str2) + 1):\n",
    "        m[x][0] = m[x-1][0] + 1\n",
    "    for y in range(1, len(str1) + 1):\n",
    "        m[0][y] = m[0][y-1] + 1\n",
    "    for x in range(1, len(str2) + 1):\n",
    "        for y in range(1, len(str1) + 1):\n",
    "            if str1[y-1] == str2[x-1]:\n",
    "                dg = 0\n",
    "            else:\n",
    "                dg = 1\n",
    "            m[x][y] = min(m[x-1][y] + 1, m[x][y-1] + 1, m[x-1][y-1] + dg)\n",
    "    return int(m[len(str2)][len(str1)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predecimos y calculamos accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inp, targ, max_samples=10):\n",
    "    \n",
    "    output_data = list()\n",
    "    i = 0\n",
    "    \n",
    "    for inputs, outputs in zip(inp, targ):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        i += 1\n",
    "        inputs = tf.expand_dims(inputs,0)\n",
    "        result = ''\n",
    "        predicted_ids = list()\n",
    "\n",
    "        hidden = [tf.zeros((1, units))]\n",
    "        enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([output_to_index['<BOS>']], 0)\n",
    "\n",
    "        for t in range(MAX_LENGTH):\n",
    "            predictions, dec_hidden, attention_weights = decoder(dec_input,dec_hidden,enc_out)\n",
    "            predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "    \n",
    "            result += index_to_output[predicted_id]\n",
    "\n",
    "            if index_to_output[predicted_id] == '<EOS>':\n",
    "                break\n",
    "\n",
    "            dec_input = tf.expand_dims([predicted_id], 0)\n",
    "            \n",
    "        output_data.append((result, predicted_ids, outputs))\n",
    "        #print(result)    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    dist = 0\n",
    "    for pred, pred_ids, gold_ids in output_data:\n",
    "        gold = \"\".join([index_to_output[i] for i in gold_ids[1:]])\n",
    "        if pred == gold:\n",
    "            correct += 1\n",
    "            print(pred, \"==\", gold)\n",
    "\n",
    "        else:\n",
    "            print(pred, \"!=\", gold)\n",
    "        total += 1\n",
    "        dist += distance(pred, gold) \n",
    "    \n",
    "    print(\"Accuracy:\", correct/total)\n",
    "    print(\"Distance:\", dist/total)   \n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos nuestra función de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([input_to_index['<BOS>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "  \n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y entrenamos nuestro modelo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "compágan<EOS> != compagináramos<EOS>\n",
      "encasquillaría<EOS> == encasquillaría<EOS>\n",
      "precalinto<EOS> != precalenté<EOS>\n",
      "aradiadas<EOS> != radiadas<EOS>\n",
      "eputearemos<EOS> != putearen<EOS>\n",
      "originastes<EOS> != originasteis<EOS>\n",
      "iaguiaras<EOS> != marginarais<EOS>\n",
      "restregarías<EOS> == restregarías<EOS>\n",
      "turnarán<EOS> == turnarán<EOS>\n",
      "acacon<EOS> != acacheteamos<EOS>\n",
      "seguin<EOS> != siguen<EOS>\n",
      "selfiara<EOS> == selfiara<EOS>\n",
      "aligeremos<EOS> != aligerásemos<EOS>\n",
      "tronchara<EOS> == tronchara<EOS>\n",
      "encalécomos<EOS> != encalabocemos<EOS>\n",
      "rentastrestuestrestuestrestuestrestuestr != rentaste<EOS>\n",
      "enhebramos<EOS> == enhebramos<EOS>\n",
      "adareis<EOS> != adarvares<EOS>\n",
      "ose<EOS> != cairelaba<EOS>\n",
      "menudease<EOS> == menudease<EOS>\n",
      "mandonea<EOS> != mandoneando<EOS>\n",
      "prolifera<EOS> != proliferaría<EOS>\n",
      "noescoce<EOS> != noescueza<EOS>\n",
      "facilitar<EOS> == facilitar<EOS>\n",
      "otoseseseseseseseseseseseseseseseseseses != tosiésemos<EOS>\n",
      "pendejeado<EOS> == pendejeado<EOS>\n",
      "satisfocoramos<EOS> != satisficiéramos<EOS>\n",
      "auspiciamos<EOS> == auspiciamos<EOS>\n",
      "cavaracavaracavaracavaracavaracavaracava != cavarais<EOS>\n",
      "rojeás<EOS> != rojeáis<EOS>\n",
      "encumbraría<EOS> == encumbraría<EOS>\n",
      "irradiseis<EOS> != irradiaseis<EOS>\n",
      "erradice<EOS> != erradica<EOS>\n",
      "recomendase<EOS> == recomendase<EOS>\n",
      "reseteaseis<EOS> != reseteasen<EOS>\n",
      "reabsorbera<EOS> != reabsorbidas<EOS>\n",
      "reargidas<EOS> != reargüidas<EOS>\n",
      "reencastes<EOS> != reencarnasteis<EOS>\n",
      "acondicionaste<EOS> == acondicionaste<EOS>\n",
      "encarecie<EOS> != encareció<EOS>\n",
      "Accuracy: 0.325\n",
      "Distance: 4.375\n",
      "Epoch 1 Loss 0.1126\n",
      "ocompagináramos<EOS> != compagináramos<EOS>\n",
      "encasquillaríaríaríaríaríaríaríaríaríarí != encasquillaría<EOS>\n",
      "precalentadoste<EOS> != precalenté<EOS>\n",
      "radiadara<EOS> != radiadas<EOS>\n",
      "eputearen<EOS> != putearen<EOS>\n",
      "originastes<EOS> != originasteis<EOS>\n",
      "marginararararararararararararararararar != marginarais<EOS>\n",
      "restregarías<EOS> == restregarías<EOS>\n",
      "tarnarán<EOS> != turnarán<EOS>\n",
      "acacheteamos<EOS> == acacheteamos<EOS>\n",
      "seguin<EOS> != siguen<EOS>\n",
      "selfiara<EOS> == selfiara<EOS>\n",
      "aligeramos<EOS> != aligerásemos<EOS>\n",
      "tronchara<EOS> == tronchara<EOS>\n",
      "encalabocomos<EOS> != encalabocemos<EOS>\n",
      "rentartustustustustustustustustustustust != rentaste<EOS>\n",
      "enhebramos<EOS> == enhebramos<EOS>\n",
      "adarvares<EOS> == adarvares<EOS>\n",
      "ose<EOS> != cairelaba<EOS>\n",
      "menudease<EOS> == menudease<EOS>\n",
      "mandoneando<EOS> == mandoneando<EOS>\n",
      "prolifararíaríaríaríaríaríaríaríaríaríar != proliferaría<EOS>\n",
      "nonace<EOS> != noescueza<EOS>\n",
      "facilitar<EOS> == facilitar<EOS>\n",
      "toseresesesesesesesesesesesesesesesesese != tosiésemos<EOS>\n",
      "pendejea<EOS> != pendejeado<EOS>\n",
      "osisfácieramos<EOS> != satisficiéramos<EOS>\n",
      "auspiciamos<EOS> == auspiciamos<EOS>\n",
      "en<EOS> != cavarais<EOS>\n",
      "rojearemos<EOS> != rojeáis<EOS>\n",
      "encumbraríaríaríaríaríaríaríaríaríaríarí != encumbraría<EOS>\n",
      "irradiases<EOS> != irradiaseis<EOS>\n",
      "erradice<EOS> != erradica<EOS>\n",
      "recomendase<EOS> == recomendase<EOS>\n",
      "reseteares<EOS> != reseteasen<EOS>\n",
      "reabsorbe<EOS> != reabsorbidas<EOS>\n",
      "reargudidara<EOS> != reargüidas<EOS>\n",
      "reencastes<EOS> != reencarnasteis<EOS>\n",
      "acondicionastocionastocionastocionastoci != acondicionaste<EOS>\n",
      "encarece<EOS> != encareció<EOS>\n",
      "Accuracy: 0.275\n",
      "Distance: 6.95\n",
      "Epoch 2 Loss 0.1862\n",
      "compagináramos<EOS> == compagináramos<EOS>\n",
      "encasquillaría<EOS> == encasquillaría<EOS>\n",
      "precalento<EOS> != precalenté<EOS>\n",
      "radiara<EOS> != radiadas<EOS>\n",
      "puteares<EOS> != putearen<EOS>\n",
      "originastististististististististististi != originasteis<EOS>\n",
      "marginarais<EOS> == marginarais<EOS>\n",
      "restregarías<EOS> == restregarías<EOS>\n",
      "turnarás<EOS> != turnarán<EOS>\n",
      "acacheteamos<EOS> == acacheteamos<EOS>\n",
      "seguiren<EOS> != siguen<EOS>\n",
      "selfiara<EOS> == selfiara<EOS>\n",
      "aligeremos<EOS> != aligerásemos<EOS>\n",
      "tronchara<EOS> == tronchara<EOS>\n",
      "encalalemos<EOS> != encalabocemos<EOS>\n",
      "testuartuastastastastastastastastastasta != rentaste<EOS>\n",
      "enhebramos<EOS> == enhebramos<EOS>\n",
      "adarvares<EOS> == adarvares<EOS>\n",
      "as<EOS> != cairelaba<EOS>\n",
      "menudease<EOS> == menudease<EOS>\n",
      "mandoneando<EOS> == mandoneando<EOS>\n",
      "prolifaría<EOS> != proliferaría<EOS>\n",
      "noceoce<EOS> != noescueza<EOS>\n",
      "facilitar<EOS> == facilitar<EOS>\n",
      "toseresesesesesesesesesesesesesesesesese != tosiésemos<EOS>\n",
      "pendejeado<EOS> == pendejeado<EOS>\n",
      "satisfácimos<EOS> != satisficiéramos<EOS>\n",
      "auspiciamos<EOS> == auspiciamos<EOS>\n",
      "ccavaras<EOS> != cavarais<EOS>\n",
      "rojeas<EOS> != rojeáis<EOS>\n",
      "encumbraría<EOS> == encumbraría<EOS>\n",
      "irradiaseis<EOS> == irradiaseis<EOS>\n",
      "erradica<EOS> == erradica<EOS>\n",
      "recomendase<EOS> == recomendase<EOS>\n",
      "reseteases<EOS> != reseteasen<EOS>\n",
      "reabsorbera<EOS> != reabsorbidas<EOS>\n",
      "rearguya<EOS> != reargüidas<EOS>\n",
      "reencastististististististististististis != reencarnasteis<EOS>\n",
      "acondicionaste<EOS> == acondicionaste<EOS>\n",
      "encarece<EOS> != encareció<EOS>\n",
      "Accuracy: 0.475\n",
      "Distance: 4.35\n",
      "Epoch 3 Loss 0.1167\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "print(\"Epoch\", epoch)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % SAVE_EACH == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    predict(dev_X, dev_Y, max_samples=40)\n",
    "\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos podido ver, los modelos sequence to sequence pueden modelar una gran variedad de problemas. Gran parte de ellos pueden aprovechar el poder de estas redes únicamente adecuando los datos de entrada para obtener los resultados deseamos. \n",
    "\n",
    "Para mejorar el rendimiento de nuestra red es posible mejorar su rendimiento con diferentes técnicas: multi task training o transfer learning.\n",
    "\n",
    "Modificaciones sugeridas para el futuro:\n",
    "* Implementar transfer learning usando datasets parecidos al español, cómo intaliano o portugués para poder mejorar el rendimiento de español.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
